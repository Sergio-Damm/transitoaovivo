# .github/workflows/atualizar-trafego.yml

name: Atualizar Dados de Tráfego

on:
  workflow_dispatch: # Adicionei os dois pontos aqui, caso faltassem novamente
  schedule:
    - cron: '*/5 * * * *' # Executa a cada 5 minutos

jobs:
  atualizar_dados:
    runs-on: ubuntu-latest
    permissions:
      contents: write # ESSENCIAL: Permite que o Action escreva no repositório

    steps:
      - name: Checkout do Repositório
        uses: actions/checkout@v4

      - name: Configurar Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x' # Garante compatibilidade

      - name: Instalar Bibliotecas Python
        run: |
          python -m pip install requests beautifulsoup4 # Instala bibliotecas para scraping
      
      # Certifica-se de que a pasta de destino para os JSONs exista
      - name: Criar Pasta de Destino para Dados JSON
        run: |
          mkdir -p assets/data # Cria o diretório se não existir

      - name: Executar Script de Scraping Python
        run: |
          python _scripts/scrape_data.py # Executa o script Python real
        shell: bash # Garante que este comando é executado em bash

      - name: Commit e Push dos Dados Atualizados
        run: |
          git config user.name 'github-actions[bot]'
          git config user.email 'github-actions[bot]@users.noreply.github.com'
          
          # Adicionar todos os arquivos JSON na pasta assets/data
          git add assets/data/*.json 
          
          # Verificar se há alterações para commitar
          if git diff --quiet --cached; then
            echo "Nenhuma alteração detectada para commitar. Ignorando commit."
          else
            git commit -m "Atualizar dados de tráfego (automatizado)"
            git push origin main
            echo "Dados de tráfego comitados e enviados com sucesso."
          fi
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}